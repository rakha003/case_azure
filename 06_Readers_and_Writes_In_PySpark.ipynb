{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37ea88a9-2990-4f0b-9342-6b70ac4df8f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Readers and Writers in PySpark\n",
    "    - In PySpark, Readers and Writers are components that enable you to read data from external sources into DataFrames or write DataFrames back to external storage\n",
    "    \n",
    "    - These components are essential for ingesting and persisting data in a distributed computing environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f5c8c00-26d2-43a7-bf66-589c2ae1c09c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Writers:\n",
    "    - Writers are responsible for writing DataFrames to external storage systems.\n",
    "    - PySpark provides various built-in writers to handle different file formats and external storage systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dd3521e-c463-46a2-b621-733092595055",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1.0. DataFrameWriter:\n",
    "    - The `DataFrameWriter` is a high-level API in PySpark that allows you to write DataFrames to various file formats and external storage systems.\n",
    "    - It is accessible through the `DataFrame.write` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "272f14fe-66e8-417d-83ec-4343214a7860",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Note**: This Notebook need DBFS Supported File System , to work and run in Databricks Env supported by DBFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5be9438-c51a-47e3-93b3-08a01964da78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1.1. JSONWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "319b5177-9d02-402d-a58c-82f80f0f639d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CreateJSONFile\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca0eb410-21cc-4577-a896-a062b8b0b089",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e8bfe9-2257-4224-87fc-9a177c149d94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample employee data\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)]\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "columns = [\"Name\", \"Age\"]\n",
    "json_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Location for Shared storage using DBFS\n",
    "outpath_empjsonFile= \"dbfs:/FileStore/shared_uploads/iriscloudone@outlook.com/emp.json\"\n",
    "\n",
    "# Save the DataFrame as a JSON file\n",
    "json_df.write.json(outpath_empjsonFile,mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc1e8ba-9236-4ace-9382-b03ec30aa0de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841c4556-7a35-4112-ac7e-509854a00f86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1.2. CSVWriter:\n",
    "   - The `csv()` method in the DataFrameWriter allows writing DataFrames to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c978d76-3ef8-4174-bcdc-6f038981d7e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample employee data\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)]\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "columns = [\"Name\", \"Age\"]\n",
    "csv_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Location for Shared storage using DBFS\n",
    "outpath_empcsvFile= \"dbfs:/FileStore/shared_uploads/iriscloudone@outlook.com/emp.csv\"\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "csv_df.write.csv(outpath_empcsvFile,header=True,mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01ce825c-a273-4ee2-a8ec-2bde1a1f69a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23187fb3-dd80-4e58-b09d-7ef7929ad0a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1.3 ParquetWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "839dad62-fa13-4356-80a3-7dead18cd532",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample employee data\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)]\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "columns = [\"Name\", \"Age\"]\n",
    "parquet_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Specify the path to save the Parquet file\n",
    "outpath_empparquetFile = \"dbfs:/FileStore/shared_uploads/iriscloudone@outlook.com/emp.parquet\"\n",
    "\n",
    "# Write the DataFrame to a Parquet file\n",
    "parquet_df.write.parquet(outpath_empparquetFile,mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6f75bb5-82ed-4156-ba79-258893746f0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquet_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8656eea1-5fb2-410b-a684-f2ac0611b735",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Readers:\n",
    "    - Readers are responsible for reading data from external sources and creating DataFrames from that data. \n",
    "    - PySpark provides various built-in readers to handle different file formats and data sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af9339cb-2cd5-46de-9c49-cf39d006f6c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.1. JSONReader:\n",
    "   - PySpark can read data from JSON files into DataFrames using the `DataFrameReader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1e10176-98fb-48b0-9a27-5b475d508d37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_json_df = spark.read.json(outpath_empjsonFile)\n",
    "output_json_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "543f53c2-4b27-4b72-9f89-7f0351e40556",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.2. CSVReader:\n",
    "   - The CSV file format is a columnar storage file format that is optimized for big data processing in Spark\n",
    "   - PySpark supports reading Parquet files through the `DataFrameReader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d643f10e-dccc-4fec-8f50-6845067ef622",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_csv_df = spark.read.csv(outpath_empcsvFile, header=True, inferSchema=True)\n",
    "output_csv_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93747018-876c-47fe-bc24-c0397b311d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.3. ParquetReader:\n",
    "   - The Parquet file format is a columnar storage file format that is optimized for big data processing in Spark\n",
    "   - PySpark supports reading Parquet files through the `DataFrameReader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61fb68c6-a8d5-42e5-a1bf-65e25bd80500",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading data from a Parquet file into a DataFrame\n",
    "output_parquet_df = spark.read.parquet(outpath_empparquetFile)\n",
    "output_parquet_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_Readers_and_Writes_In_PySpark",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
